{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d6b19",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba092ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, \\\n",
    "                            recall_score, accuracy_score, classification_report\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from skimage import io, exposure, morphology, filters, color, \\\n",
    "                    segmentation, feature, measure, img_as_float, img_as_ubyte\n",
    "from skimage.color.adapt_rgb import adapt_rgb, each_channel, hsv_value\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, \\\n",
    "                         Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "dataset_default_path = './dataset'\n",
    "train_set_path = './train'\n",
    "test_set_path = './test'\n",
    "weights_filename = \"./alexnet_weights.h5\"\n",
    "\n",
    "rand_seed = 0\n",
    "\n",
    "# Number of epochs, where each epoch represents one forward pass and one backward pass\n",
    "# of all the training samples\n",
    "num_epochs = 75\n",
    "# the batch size defines the number of samples that will be propagated through the network.\n",
    "# In other words it represents the number of training examples in one forward/backward pass.\n",
    "# We will use the mini-batch approach instead of using the whole training set for each pass.\n",
    "batch_size = 50\n",
    "# shape of the dataset images: length = 450 px, height = 450 px, channels = 3 (RGB)\n",
    "img_shape = (600, 450, 3)\n",
    "\n",
    "# Minimum number of images of HAM10000 dataset to use \n",
    "min_samples = 200\n",
    "# Maximum number of images (and their info) that will be printed during\n",
    "# preprocessing, segmentation and feature extraction.\n",
    "# This is needed to avoid the error message: \n",
    "# \"Buffered data was truncated after reaching the output size limit\"\n",
    "# because there is a limited memory for displaying output of a cell on Colab\n",
    "plot_limit = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfa405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(image, size=None, cmap=None, no_axis=True):\n",
    "    \"\"\"\n",
    "    Plot a single image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ndarray\n",
    "        Image to be plotted.\n",
    "    size : tuple of in (length, height)\n",
    "        Sizes of the figure.\n",
    "    cmap : str\n",
    "        Figure colormap.\n",
    "    no_axis : bool\n",
    "        Whether or not to show axis\n",
    "    \"\"\"\n",
    "    if size is None:\n",
    "        size = plt.rcParams['figure.figsize']\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=size)\n",
    "    ax.grid(False)\n",
    "    if cmap is not None:\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "    else:\n",
    "        ax.imshow(image)\n",
    "    if no_axis:\n",
    "        ax.axis('off')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_all(*images, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot a series of images side-by-side.\n",
    "\n",
    "    Convert all images to float so that images have a common intensity range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    limits : str\n",
    "        Control the intensity limits. By default, 'image' is used set the\n",
    "        min/max intensities to the min/max of all images. Setting `limits` to\n",
    "        'dtype' can also be used if you want to preserve the image exposure.\n",
    "    titles : list of str\n",
    "        Titles for subplots. If the length of titles is less than the number\n",
    "        of images, empty strings are appended.\n",
    "    kwargs : dict\n",
    "        Additional keyword-arguments passed to `imshow`.\n",
    "    \"\"\"\n",
    "    images = [img_as_float(img) for img in images]\n",
    "\n",
    "    titles = kwargs.pop('titles', [])\n",
    "    if len(titles) != len(images):\n",
    "        titles = list(titles) + [''] * (len(images) - len(titles))\n",
    "\n",
    "    limits = kwargs.pop('limits', 'image')\n",
    "    if limits == 'image':\n",
    "        kwargs.setdefault('vmin', min(img.min() for img in images))\n",
    "        kwargs.setdefault('vmax', max(img.max() for img in images))\n",
    "    elif limits == 'dtype':\n",
    "        vmin, vmax = dtype_limits(images[0])\n",
    "        kwargs.setdefault('vmin', vmin)\n",
    "        kwargs.setdefault('vmax', vmax)\n",
    "\n",
    "    nrows, ncols = kwargs.get('shape', (1, len(images)))\n",
    "    \n",
    "    axes_off = kwargs.pop('axes_off', False)\n",
    "\n",
    "    size = nrows * kwargs.pop('size', 5)\n",
    "    width = size * len(images)\n",
    "    if nrows > 1:\n",
    "        width /= nrows * 1.33\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(width, size))\n",
    "    for ax, img, label in zip(axes.ravel(), images, titles):\n",
    "        ax.imshow(img, **kwargs)\n",
    "        ax.set_title(label)\n",
    "        ax.grid(False)\n",
    "        if axes_off:\n",
    "            ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_with_histogram(image, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot an image side-by-side with its histogram.\n",
    "\n",
    "    - Plot the image next to the histogram\n",
    "    - Plot each RGB channel separately (if input is color)\n",
    "    - Automatically flatten channels\n",
    "    - Select reasonable bins based on the image's dtype\n",
    "    \"\"\"\n",
    "    # create a new subplot made of 2 figures\n",
    "    width, height = plt.rcParams['figure.figsize']\n",
    "    fig, (ax_image, ax_hist) = plt.subplots(ncols=2, figsize=(2*width, height))\n",
    "    \n",
    "    titles = kwargs.pop('titles', [])\n",
    "    if len(titles) == 2:\n",
    "        ax_image.set_title(titles[0])\n",
    "        ax_hist.set_title(titles[1])\n",
    "\n",
    "    # first plot the image\n",
    "    kwargs.setdefault('cmap', plt.cm.gray)\n",
    "    ax_image.imshow(image, **kwargs)\n",
    "    \n",
    "    # then plot its histogram\n",
    "    if image.ndim == 2:\n",
    "        hist, bin_centers = exposure.histogram(image)\n",
    "        ax_hist.fill_between(bin_centers, hist, alpha=0.3, color='black', **kwargs)\n",
    "    elif image.ndim == 3:\n",
    "        # `channel` is the red, green, or blue channel of the image.\n",
    "        for channel, channel_color in zip([channel for channel in np.rollaxis(image, -1)], 'rgb'):\n",
    "            hist, bin_centers = exposure.histogram(channel)\n",
    "            ax_hist.fill_between(bin_centers, hist, alpha=0.3, color=channel_color, **kwargs)\n",
    "    ax_hist.set_xlabel('intensity')\n",
    "    ax_hist.set_ylabel('# pixels')\n",
    "    # ax_hist.grid(False)\n",
    "\n",
    "    ax_image.set_axis_off()\n",
    "    # match axes height\n",
    "    plt.draw()\n",
    "    dst = ax_hist.get_position()\n",
    "    src = ax_image.get_position()\n",
    "    ax_hist.set_position([dst.xmin, src.ymin, dst.width, src.height])\n",
    "    return ax_image, ax_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "structuring_element = morphology.disk(7)\n",
    "\n",
    "@adapt_rgb(each_channel)\n",
    "def morph_closing_each(image, struct_element):\n",
    "    return morphology.closing(image, struct_element)\n",
    "\n",
    "\n",
    "@adapt_rgb(hsv_value)\n",
    "def morph_closing_hsv(image, struct_element):\n",
    "    return morphology.closing(image, struct_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@adapt_rgb(each_channel)\n",
    "def median_filter_each(image, struct_element):\n",
    "    return filters.median(image, struct_element)\n",
    "\n",
    "\n",
    "@adapt_rgb(hsv_value)\n",
    "def median_filter_hsv(image, struct_element):\n",
    "    return filters.median(image, struct_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test(num_samples=0, train_set_frac=0.75, dataset_path=dataset_default_path,\n",
    "                     train_path=train_set_path, test_path=test_set_path,\n",
    "                     overwrite=False):\n",
    "    \"\"\"\n",
    "    Build the training and test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples : int\n",
    "        Number of samples to select from the dataset. If zero or less than zero,\n",
    "        all the samples in the dataset will be chosen.\n",
    "        If higher than the dataset size it will be set to the dataset size.\n",
    "    train_set_frac : float\n",
    "        It represents the proportion of the dataset to include in the train split.\n",
    "        If less than 0.0 or greater than 1.0 it will be set to the default value (0.75).\n",
    "    dataset_path : str\n",
    "        It should point to a valid path containing the images of the HAM10000 dataset.\n",
    "        If not valid it will be set to the default path './dataset'.\n",
    "    train_path : str\n",
    "        It should point to a valid path to store images of the training set.\n",
    "        If not valid it will be set to the default path './train'.\n",
    "    test_path : str\n",
    "        It should point to a valid path to store images of the test set.\n",
    "        If not valid it will be set to the default path './test'.\n",
    "    overwrite : bool\n",
    "        Specifies whether or not to overwrite the directories related to training\n",
    "        and test set.\n",
    "    \n",
    "    Returns training set (DataFrame), test set (DataFrame) and class labels\n",
    "    \"\"\"\n",
    "    # if 'dataset_path' is not valid set to the default path ('./dataset')\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        print('Warning: {} not found. We will use {} for the dataset'.format(\n",
    "              dataset_path, dataset_default_path))\n",
    "        dataset_path = dataset_default_path\n",
    "    # Get the absolute path of the dataset directory\n",
    "    abs_dataset_path = os.path.abspath(dataset_path)\n",
    "\n",
    "    # if 'train_path' is not valid set to the default path ('./train')\n",
    "    if not os.path.isdir(train_path):\n",
    "        print('Warning: {} not found. We will use {} to store the training set'.format(\n",
    "              train_path, train_set_path))\n",
    "        train_path = train_set_path\n",
    "    # Get the absolute path of the training set\n",
    "    abs_train_path = os.path.abspath(train_path)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(abs_train_path)\n",
    "        os.mkdir(abs_train_path)\n",
    "\n",
    "    # If 'test_path' is not valid set to the default path ('./test')\n",
    "    if not os.path.isdir(test_path):\n",
    "        print('Warning: {} not found. We will use {} to store the test set'.format(\n",
    "              test_path, test_set_path))\n",
    "        test_path = test_set_path\n",
    "    # Get the absolute path of the test set\n",
    "    abs_test_path = os.path.abspath(test_path)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(abs_test_path)\n",
    "        os.mkdir(abs_test_path)\n",
    "\n",
    "    # Read the dataset metadata from HAM10000_metadata.csv as a Pandas dataframe\n",
    "    ham10000_df = pd.read_csv('HAM10000_metadata.csv')\n",
    "    \n",
    "    # For each image name in the 'image_id' column add the '.jpg' suffix.\n",
    "    # This is needed by flow_from_dataframe since each item in 'image_id' column\n",
    "    # must specify the (relative) path of the corresponding image\n",
    "    # (see 'flow_from_dataframe' documentation)\n",
    "    for img_idx in range(ham10000_df['image_id'].count()):\n",
    "        ham10000_df.at[img_idx, 'image_id'] += '.jpg'\n",
    "\n",
    "    # Get class labels\n",
    "    classes = list(set(ham10000_df.dx))\n",
    "    print('Class labels: {}'.format(classes))\n",
    "\n",
    "    # Get and print the number of images in the dataset\n",
    "    num_images = ham10000_df.image_id.count()\n",
    "    print('Number of images in the HAM10000 dataset: {}'.format(num_images))\n",
    "\n",
    "    if 0 < num_samples < min_samples:\n",
    "        print('Warning: The chosen number of samples is low. '\n",
    "              'It will be set by default to {}'.format(min_samples))\n",
    "        ham10000_df = ham10000_df.sample(n=min_samples, random_state=rand_seed)\n",
    "        print('Number of images in the reduced dataset: {}'.format(min_samples))\n",
    "    elif num_samples > num_images:\n",
    "        print('Warning: The chosen number of samples is greater than the dataset size. '\n",
    "              'The dataset size will not be changed')\n",
    "    elif min_samples <= num_samples <= num_images:\n",
    "        ham10000_df = ham10000_df.sample(n=num_samples, random_state=rand_seed)\n",
    "        print('Number of images in the reduced dataset: {}'.format(num_samples))\n",
    "\n",
    "    # Print the number of images in each class (dataset)\n",
    "    print('Number of images for each class (dataset):')\n",
    "    print(ham10000_df.dx.value_counts())\n",
    "\n",
    "    if train_set_frac < 0 or train_set_frac > 1.0:\n",
    "        test_set_frac = 0.75\n",
    "\n",
    "    # Randomly split the images into training and test set such that the\n",
    "    # training set has 75% of the images of the dataset (and so the remaining\n",
    "    # 25% of the dataset images will be put in the test set).\n",
    "    # In addition, since the dataset is unbalanced (see the statistics printed above),\n",
    "    # we use the 'stratify' parameter in order to split the dataset in a\n",
    "    # stratified fashion (the training and test sets will contain the same proportion\n",
    "    # of samples as the dataset for each class label)\n",
    "    train_set, test_set = train_test_split(ham10000_df, train_size=train_set_frac,\n",
    "                                           random_state=rand_seed,\n",
    "                                           stratify=ham10000_df.dx)\n",
    "\n",
    "    print('\\nNumber of images in the training set : {}'.format(train_set.image_id.count()))\n",
    "    print('Number of images for each class (training set):')\n",
    "    print(train_set.dx.value_counts())\n",
    "    print('\\nNumber of images in the test set : {}'.format(test_set.image_id.count()))\n",
    "    print('Number of images for each class (test set):')\n",
    "    print(test_set.dx.value_counts())\n",
    "\n",
    "    # Get the name of images in the training and test sets\n",
    "    train_imgs = list(train_set['image_id'])\n",
    "    test_imgs = list(test_set['image_id'])\n",
    "\n",
    "    # Move the images designated for the training set into the related directory\n",
    "    for img_name in train_imgs:\n",
    "        src_path = os.path.join(abs_dataset_path, img_name)\n",
    "        shutil.copy2(src_path, abs_train_path)\n",
    "\n",
    "    # Move the images designated for the training set into the related directory\n",
    "    for img_name in test_imgs:\n",
    "        src_path = os.path.join(abs_dataset_path, img_name)\n",
    "        shutil.copy2(src_path, abs_test_path)\n",
    "  \n",
    "    # Return the training set (DataFrame), the test set (DataFrame) and class labels\n",
    "    return train_set, test_set, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import img_as_ubyte\n",
    "def preprocessing(train_path=train_set_path, test_path=test_set_path):\n",
    "    \"\"\"\n",
    "    Apply some image processing techniques (CLAHE, morphology closing and median\n",
    "    filter) to images of training and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_path : str\n",
    "        Training set path.\n",
    "        If not valid it will be set to the default path './train'.\n",
    "    test_path : str\n",
    "        Test set path.\n",
    "        If not valid it will be set to the default path './test'.\n",
    "    \n",
    "    Returns training set (DataFrame), test set (DataFrame) and class labels\n",
    "    \"\"\"\n",
    "\n",
    "    print('-- PREPROCESSING --')\n",
    "\n",
    "    # if 'train_path' is not valid set to the default path ('./train')\n",
    "    if not os.path.isdir(train_path):\n",
    "        train_path = train_set_path\n",
    "    # get absolute path of the training set\n",
    "    abs_train_path = os.path.abspath(train_path)\n",
    "\n",
    "    # if 'test_path' is not valid set to the default path ('./test')\n",
    "    if not os.path.isdir(test_path):\n",
    "        test_path = test_set_path\n",
    "    # get absolute path of the test set\n",
    "    abs_test_path = os.path.abspath(test_path)\n",
    "\n",
    "    # list of images in the training set\n",
    "    img_train = [img_file for img_file in os.listdir(abs_train_path)\n",
    "                 if os.path.isfile(os.path.join(abs_train_path, img_file)) and img_file.endswith('.jpg')]\n",
    "    # list of images in the test set\n",
    "    img_test = [img_file for img_file in os.listdir(abs_test_path)\n",
    "                if os.path.isfile(os.path.join(abs_test_path, img_file)) and img_file.endswith('.jpg')]\n",
    "    img_list = img_train + img_test\n",
    "\n",
    "    for idx, image_name in enumerate(img_list):\n",
    "        # get image path\n",
    "        if image_name in img_train:\n",
    "            image_path = os.path.join(abs_train_path, image_name)\n",
    "        elif image_name in img_test:\n",
    "            image_path = os.path.join(abs_test_path, image_name)\n",
    "        else:\n",
    "            print('Error: Cannot find {}'.format(image_name))\n",
    "            return\n",
    "        # read the image\n",
    "        image = io.imread(image_path)\n",
    "\n",
    "        if idx < plot_limit:\n",
    "            # plot basic image info\n",
    "            print('{:_<100}'.format(''))\n",
    "            print('Image name: {}'.format(image_name))\n",
    "            print('Image sizes: {}'.format(image.shape))\n",
    "            print('Image data type: {}'.format(image.dtype))\n",
    "        elif idx == plot_limit:\n",
    "            print('Continuing image processing without printing the results ...')\n",
    "    \n",
    "        # 1) Equalize the image contrast using  the Contrast-Limited Adaptive \n",
    "        #    Histogram Equalization (CLAHE) method.\n",
    "        equalized_adapthist = exposure.equalize_adapthist(image)\n",
    "    \n",
    "        # 2) Use morphological closing to remove hairs (in order to simulate the Razor filter).\n",
    "        #    Images are processed using the 'morph_closing_each()' function which applies\n",
    "        #    the morphological closing to each channel of the RGB image.\n",
    "        img_morph_closing = morph_closing_each(equalized_adapthist, structuring_element)\n",
    "    \n",
    "        # 3) Use the median filter to apply interpolation on the obtained image.\n",
    "        #    The median filter is implemented using the 'median_filter_each()'\n",
    "        #    function which pass each of the RGB channels to the median filter\n",
    "        #    one-by-one, and stitch the results back into an RGB image.\n",
    "        img_filtered = median_filter_each(img_morph_closing, structuring_element)\n",
    "    \n",
    "        if idx < plot_limit:\n",
    "            # plot the original image with its histogram\n",
    "            imshow_with_histogram(image, titles=['Original image', 'Histogram'])\n",
    "\n",
    "            # plot the image obtained after applying the CLAHE method\n",
    "            imshow_with_histogram(equalized_adapthist,\n",
    "                                  titles=['Image equalized (CLAHE)', 'Histogram'])\n",
    "\n",
    "            # plot the images obtained using the morphological closing and the one\n",
    "            # obtained after applying the median filter\n",
    "            imshow_all(img_morph_closing, img_filtered, axes_off=True,\n",
    "                       titles=['Morphological Closing', 'Median Filter'])\n",
    "            plt.show();\n",
    "\n",
    "        # save the final processed image\n",
    "        io.imsave(image_path,img_as_ubyte(img_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d45b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import watershed\n",
    "def images_segmentation(train_path=train_set_path, test_path=test_set_path):\n",
    "\n",
    "    print('-- REGION-BASED SEGMENTATION --')\n",
    "    \n",
    "    # Python dictionaries containing the properties of the regions representing\n",
    "    # the lesions (subdivided into training and test set)\n",
    "    segmented_train_set = {}\n",
    "    segmented_test_set = {}\n",
    "    \n",
    "    if not os.path.isdir(train_path):\n",
    "        train_path = train_set_path\n",
    "    # get absolute path of the training set\n",
    "    abs_train_path = os.path.abspath(train_path)\n",
    "\n",
    "    if not os.path.isdir(test_path):\n",
    "        test_path = test_set_path\n",
    "    # get absolute path of the test set\n",
    "    abs_test_path = os.path.abspath(test_path)\n",
    "\n",
    "    # list of images in the training set\n",
    "    train_imgs = [img_file for img_file in os.listdir(abs_train_path)\n",
    "                  if os.path.isfile(os.path.join(abs_train_path, img_file)) and img_file.endswith('.jpg')]\n",
    "    # list of images in the test set\n",
    "    test_imgs = [img_file for img_file in os.listdir(abs_test_path)\n",
    "                 if os.path.isfile(os.path.join(abs_test_path, img_file)) and img_file.endswith('.jpg')]\n",
    "\n",
    "    img_list = train_imgs + test_imgs\n",
    "\n",
    "    for idx, image_name in enumerate(img_list):\n",
    "        if idx < plot_limit:\n",
    "            print('{:_<100}'.format(''))\n",
    "            print('Image name: {}'.format(image_name))\n",
    "\n",
    "        # get image path\n",
    "        if image_name in train_imgs:\n",
    "            image_path = os.path.join(abs_train_path, image_name)\n",
    "        elif image_name in test_imgs:\n",
    "            image_path = os.path.join(abs_test_path, image_name)\n",
    "        else:\n",
    "            print('Error: Cannot find {}'.format(image_name))\n",
    "            return None, None\n",
    "        # read image\n",
    "        image = io.imread(image_path)\n",
    "        # convert the original image into grayscale\n",
    "        gray_img = color.rgb2gray(image)\n",
    "\n",
    "        # 1] Apply Sobel filter\n",
    "        elevation_map = filters.sobel(gray_img)\n",
    "\n",
    "        # 2] Build image markers using the threshold obtained through the ISODATA filter\n",
    "        markers = np.zeros_like(gray_img)\n",
    "        threshold = filters.threshold_isodata(gray_img)\n",
    "        markers[gray_img > threshold] = 1\n",
    "        markers[gray_img < threshold] = 2\n",
    "        \n",
    "        # 3] Apply Wathershed algorithm in order to segment the image filtered\n",
    "        #    using the markers\n",
    "        segmented_img = watershed(elevation_map, markers)\n",
    "        # 4] Improve segmantation:\n",
    "        #    >  Fill small holes \n",
    "        segmented_img = ndi.binary_fill_holes(segmented_img - 1)\n",
    "        #    > Remove small objects that have an area less than 800 px:\n",
    "        #      this could be useful to exclude some regions that does not represent a lesion\n",
    "        segmented_img = morphology.remove_small_objects(segmented_img, min_size=800)\n",
    "        #    > Clear regions connected to the image borders.\n",
    "        #      This operation is very useful when there are contour regions have a\n",
    "        #      big area and so they can be exchanged with the lesion.\n",
    "        #      However, this can also create some issues when the lesion region is\n",
    "        #      connected to the image borders. In order to (try to) overcome this\n",
    "        #      issue, we use a lesion identification algorithm (see below)\n",
    "        img_border_cleared = segmentation.clear_border(segmented_img)\n",
    "\n",
    "        # 5] Apply connected components labeling algorithm:\n",
    "        #    it assigns labels to a pixel such that adjacent pixels of the same\n",
    "        #    features are assigned the same label.\n",
    "        # labeled_img, _ = ndi.label(segmented_img)\n",
    "        labeled_img = morphology.label(img_border_cleared)\n",
    "\n",
    "        if idx < plot_limit:\n",
    "            # create a subplot of 3 figures in order to show elevation map,\n",
    "            # markers and the segmanted image\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(10, 8))\n",
    "            ax[0].imshow(elevation_map, cmap=plt.cm.gray)\n",
    "            ax[0].set_title('elevation map')\n",
    "            ax[0].set_axis_off()\n",
    "\n",
    "            ax[1].imshow(markers, cmap=plt.cm.nipy_spectral)\n",
    "            ax[1].set_title('markers')\n",
    "            ax[1].set_axis_off()\n",
    "\n",
    "            ax[2].imshow(segmented_img, cmap=plt.cm.gray)\n",
    "            ax[2].set_title('segmentation')\n",
    "            ax[2].set_axis_off()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show();\n",
    "        \n",
    "        # 6] Lesion identification algorithm:\n",
    "        # Compute properties of labeled image regions:\n",
    "        # it will be used to automatically select the region that contains\n",
    "        # the skin lesion according to area and extent\n",
    "        props = measure.regionprops(labeled_img)\n",
    "        # num labels -> num regions\n",
    "        num_labels = len(props)\n",
    "        # Get all the area of detected regions\n",
    "        areas = [region.area for region in props]\n",
    "\n",
    "        # If we have at least one region and the area of the region having the\n",
    "        # biggest area is at least 1200 px, we choose it as the region that\n",
    "        # contains the leson because if properly segmented (i.e., after removing\n",
    "        # small objects and regions on the image contours (since in most of the\n",
    "        # images, the lesion is in the center))\n",
    "        if num_labels > 0 and areas[np.argmax(areas)] >= 1200:\n",
    "            if idx < plot_limit:\n",
    "                print('Num labels:', num_labels)\n",
    "                print('Areas: {}'.format(areas))\n",
    "            target_label = props[np.argmax(areas)].label\n",
    "        else:\n",
    "            # ... otherwise we could have one of the following two cases:\n",
    "            # 1] num_labels == 0:\n",
    "            #    this can happen when there is only one region (the one containing the lesion)\n",
    "            #    but it has been deleted when applying the function segmentation.clear_border()\n",
    "            # 2] num_labels > 0 but areas[np.argmax(areas)] < 1200:\n",
    "            #    it means that there exists at least one region but all the regions have\n",
    "            #    an area less than 1200 pixels.\n",
    "            #    This can happen when the region containing the lesion is deleted\n",
    "            #    with segmentation.clear_border() but there still other regions that\n",
    "            #    could be \"exhanged for\" a lesion region\n",
    "            #\n",
    "            # Since both cases can be due to the deletion of the segmented area\n",
    "            # because of the use of segmentation.clear_border(), the idea is to\n",
    "            # backtrack to the original segmented image (the one obtained \n",
    "            # obtained before applying segmentation.clear_border()), apply again the\n",
    "            # connected components labeling algorithm and extract the new region properties.\n",
    "            # In addition, in order to find the region representing the lesion,\n",
    "            # we use area and extent features by checking among the three largest regions\n",
    "            # (if any because there could be only one or two regions) sorted in ascending order\n",
    "            # (i.e. from the one having the largest area) the first that has an extent grater than 0.5.\n",
    "            labeled_img = morphology.label(segmented_img)\n",
    "            # Get new region properties\n",
    "            props = measure.regionprops(labeled_img)\n",
    "            # Get the new list of areas\n",
    "            areas = [region.area for region in props]\n",
    "            # List of regions' extent.\n",
    "            # Each extent is defined as the ratio of pixels in the region  to pixels\n",
    "            # in the total bounding box (computed as: area / (rows * cols))\n",
    "            extents = [region.extent for region in props]\n",
    "            if idx < plot_limit:\n",
    "                print('Num labels: {}'.format(len(props)))\n",
    "                print('Areas: {}'.format(areas))\n",
    "                print('Extents: {}'.format(extents))\n",
    "            # Get the index of the region having the largest area and if there are\n",
    "            # more than one or two regions, find also the index of the second and\n",
    "            # third most largest regions.\n",
    "            region_max1 = np.argmax(areas)\n",
    "            if len(props) > 1:\n",
    "                areas_copy = areas.copy()\n",
    "                areas_copy[region_max1] = 0\n",
    "                region_max2 = np.argmax(areas_copy)\n",
    "            if len(props) > 2:\n",
    "                areas_copy[region_max2] = 0\n",
    "                region_max3 = np.argmax(areas_copy)\n",
    "\n",
    "            # If the largest region has an extent greater than 0.50, it is our target region\n",
    "            if extents[region_max1] > 0.50:\n",
    "                target_label = props[region_max1].label\n",
    "            # ... else check if the extent of the second largest region is greater than 0.5,\n",
    "            # and if so we have found our target region\n",
    "            elif len(props) > 1 and extents[region_max2] > 0.50:\n",
    "                target_label = props[region_max2].label\n",
    "            # ... else if the third largest region has an extent greater than 0.50,\n",
    "            # it is (more probably) the one containing the lesion\n",
    "            elif len(props) > 2 and extents[region_max3] > 0.50:\n",
    "                target_label = props[region_max3].label\n",
    "            # ... otherwise we choose the largest region\n",
    "            else:\n",
    "                target_label = props[region_max1].label\n",
    "\n",
    "            # NOTE: another possible approarch could be to select as the target region\n",
    "            #       the one having the largest extent among the 3 largest regions found:\n",
    "            # if len(props) > 2:\n",
    "            #     extents_largest_reg = [val if idx in (region_max1, region_max2, region_max3) else 0.0\n",
    "            #                            for idx, val in enumerate(extents)]\n",
    "            # elif len(props) > 1:\n",
    "            #     extents_largest_reg = [val if idx in (region_max1, region_max2) else 0.0\n",
    "            #                            for idx, val in enumerate(extents)]\n",
    "            # else:\n",
    "            #     extents_largest_reg = [val if idx in (region_max1,) else 0.0\n",
    "            #                            for idx, val in enumerate(extents)]\n",
    "            # target_label = props[np.argmax(extents_largest_reg)].label\n",
    "\n",
    "        # assign label 0 to all the pixels that are not in the target region (that is\n",
    "        # the ragion that more probably contains the lesion)\n",
    "        for row, col in np.ndindex(labeled_img.shape):\n",
    "            if labeled_img[row, col] != target_label:\n",
    "                labeled_img[row, col] = 0\n",
    "        # Convert the labeled image into its RGB version\n",
    "        image_label_overlay = color.label2rgb(labeled_img, gray_img)\n",
    "\n",
    "        if idx < plot_limit:\n",
    "            print('Chosen label: {}'.format(target_label))\n",
    "            # Plot the original image ('image') in which the contours of all the\n",
    "            # segmented regions are highlighted\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(8, 6), sharey=True)\n",
    "            axes[0].imshow(image)\n",
    "            axes[0].contour(segmented_img, [0.5], linewidths=1.2, colors='y')\n",
    "            axes[0].axis('off')\n",
    "            # Plot 'image_label_overlay' that contains the target region highlighted\n",
    "            axes[1].imshow(image_label_overlay)\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show();\n",
    "        elif idx == plot_limit:\n",
    "            print('Continuing segmentation without printing the results ...')\n",
    "\n",
    "        # Add the the found region into the proper dictionary according to whether\n",
    "        # the current image belongs to the training or the test set\n",
    "        if image_name in train_imgs:\n",
    "            segmented_train_set[image_name] = props[target_label - 1]\n",
    "        elif image_name in test_imgs:\n",
    "            segmented_test_set[image_name] = props[target_label - 1]\n",
    "\n",
    "    return segmented_train_set, segmented_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extraction(segmented_region_train, segmented_region_test, out_df=False,\n",
    "                        train_path=train_set_path, test_path=test_set_path):\n",
    "\n",
    "    print('-- FEATURE EXTRACTION --')\n",
    "\n",
    "    if out_df:\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "        train_index = []\n",
    "        test_index = []\n",
    "    else:\n",
    "        train = {}\n",
    "        test = {}\n",
    "\n",
    "    if not os.path.isdir(train_path):\n",
    "        train_path = train_set_path\n",
    "    # absolute path of the training set\n",
    "    abs_train_path = os.path.abspath(train_path)\n",
    "  \n",
    "    # absolute path of the test set\n",
    "    if not os.path.isdir(test_path):\n",
    "        test_path = test_set_path\n",
    "    abs_test_path = os.path.abspath(test_path)\n",
    "\n",
    "    segmented_regions = {**segmented_region_train, **segmented_region_test}  # Python 3.5\n",
    "    # segmented_regions = {k: v for d in (segmented_region_train, segmented_region_test) for k, v in d.items()}\n",
    "    for idx, image_name in enumerate(segmented_regions):\n",
    "        if idx < plot_limit:\n",
    "            print('{:_<100}'.format(''))\n",
    "            print('Image name: {}'.format(image_name))\n",
    "        elif idx == plot_limit:\n",
    "            print('\\nContinuing feature extraction without printing the results ...')\n",
    "\n",
    "        if image_name in segmented_region_train:\n",
    "            image_path = os.path.join(abs_train_path, image_name)\n",
    "        elif image_name in segmented_region_test:\n",
    "            image_path = os.path.join(abs_test_path, image_name)\n",
    "        else:\n",
    "            print('Error: Cannot find {}'.format(image_name))\n",
    "            return None\n",
    "\n",
    "        image = io.imread(image_path)\n",
    "        gray_img = color.rgb2gray(image)\n",
    "\n",
    "        lesion_region = segmented_regions[image_name]\n",
    "\n",
    "        # 1] ASYMMETRY\n",
    "        area_total = lesion_region.area\n",
    "        img_mask = lesion_region.image\n",
    "\n",
    "        horizontal_flip = np.fliplr(img_mask)\n",
    "        diff_horizontal = img_mask * ~horizontal_flip\n",
    "\n",
    "        vertical_flip = np.flipud(img_mask)\n",
    "        diff_vertical = img_mask * ~vertical_flip\n",
    "\n",
    "        diff_horizontal_area = np.count_nonzero(diff_horizontal)\n",
    "        diff_vertical_area = np.count_nonzero(diff_vertical)\n",
    "        asymm_idx = 0.5 * ((diff_horizontal_area / area_total) + (diff_vertical_area / area_total))\n",
    "        ecc = lesion_region.eccentricity\n",
    "        # mmr = lesion_region.minor_axis_length / lesion_region.major_axis_length\n",
    "\n",
    "        if idx < plot_limit:\n",
    "            print('-- ASYMMETRY --')\n",
    "            print('Diff area horizontal: {:.3f}'.format(np.count_nonzero(diff_horizontal)))\n",
    "            print('Diff area vertical: {:.3f}'.format(np.count_nonzero(diff_vertical)))\n",
    "            print('Asymmetric Index: {:.3f}'.format(asymm_idx))\n",
    "            print('Eccentricity: {:.3f}'.format(ecc))\n",
    "            # print('Minor-Major axis ratio: {}'.format(mmr))\n",
    "        \n",
    "            imshow_all(img_mask, horizontal_flip, diff_horizontal,\n",
    "                       titles=['image mask', 'horizontal flip', 'difference'], size=4, cmap='gray')\n",
    "            imshow_all(img_mask, vertical_flip, diff_vertical,\n",
    "                       titles=['image mask', 'vertical flip', 'difference'], size=4, cmap='gray')\n",
    "            plt.show();\n",
    "\n",
    "        # 2] Border irregularity:\n",
    "        compact_index = (lesion_region.perimeter ** 2) / (4 * np.pi * area_total)\n",
    "        if idx < plot_limit:\n",
    "            print('\\n-- BORDER IRREGULARITY --')\n",
    "            print('Compact Index: {:.3f}'.format(compact_index))\n",
    "\n",
    "        # 3] Color variegation:\n",
    "        sliced = image[lesion_region.slice]\n",
    "        lesion_r = sliced[:, :, 0]\n",
    "        lesion_g = sliced[:, :, 1]\n",
    "        lesion_b = sliced[:, :, 2]\n",
    "\n",
    "        C_r = np.std(lesion_r) / np.max(lesion_r)\n",
    "        C_g = np.std(lesion_g) / np.max(lesion_g)\n",
    "        C_b = np.std(lesion_b) / np.max(lesion_b)\n",
    "\n",
    "        if idx < plot_limit:\n",
    "            print('\\n-- COLOR VARIEGATION --')\n",
    "            print('Red Std Deviation: {:.3f}'.format(C_r))\n",
    "            print('Green Std Deviation: {:.3f}'.format(C_g))\n",
    "            print('Blue Std Deviation: {:.3f}'.format(C_b))\n",
    "            # imshow_all(lesion_r, lesion_g, lesion_b)\n",
    "            # plt.show();\n",
    "\n",
    "        # Alternative method to compute colorfulness:\n",
    "        # https://www.pyimagesearch.com/2017/06/05/computing-image-colorfulness-with-opencv-and-python/\n",
    "        # compute rg = Red - Green\n",
    "        # rg = np.absolute(lesion_r - lesion_g)\n",
    "        # compute yb = 0.5 * (Red + Green) - Blue\n",
    "        # yb = np.absolute(0.5 * (lesion_r + lesion_g) - lesion_b)\n",
    "        #\n",
    "        # compute the mean and standard deviation of both `rg` and `yb`\n",
    "        # (rb_mean, rb_std) = (np.mean(rg), np.std(rg))\n",
    "        # (yb_mean, yb_std) = (np.mean(yb), np.std(yb))\n",
    "        #\n",
    "        # combine the mean and standard deviations\n",
    "        # std_root = np.sqrt((rb_std ** 2) + (yb_std ** 2))\n",
    "        # mean_root = np.sqrt((rb_mean ** 2) + (yb_mean ** 2))\n",
    "        #\n",
    "        # derive the \"colorfulness\" metric (color index)\n",
    "        # color_index = std_root + (0.3 * mean_root)\n",
    "\n",
    "        # 4] Diameter:\n",
    "        eq_diameter = lesion_region.equivalent_diameter\n",
    "        if idx < plot_limit:\n",
    "            print('\\n-- DIAMETER --')\n",
    "            print('Equivalent diameter: {:.3f}'.format(eq_diameter))\n",
    "            # optionally convert the diameter in mm, knowing that 1 px = 0.265 mm:\n",
    "            # 1 px : 0.265 mm = diam_px : diam_mm -> diam_mm = diam_px * 0.265\n",
    "            print('Diameter (mm): {:.3f}'.format(eq_diameter * 0.265))\n",
    "\n",
    "        # 5] Texture:\n",
    "        glcm = feature.greycomatrix(image=img_as_ubyte(gray_img), distances=[1],\n",
    "                                    angles=[0, np.pi/4, np.pi/2, np.pi * 3/2],\n",
    "                                    symmetric=True, normed=True)\n",
    "\n",
    "        correlation = np.mean(feature.greycoprops(glcm, prop='correlation'))\n",
    "        homogeneity = np.mean(feature.greycoprops(glcm, prop='homogeneity'))\n",
    "        energy = np.mean(feature.greycoprops(glcm, prop='energy'))\n",
    "        contrast = np.mean(feature.greycoprops(glcm, prop='contrast'))\n",
    "        dissimilarity = np.mean(feature.graycoprops(glcm, 'dissimilarity'))\n",
    "        ASM=np.mean(feature.graycoprops(glcm, 'ASM'))\n",
    "        \n",
    "\n",
    "        if idx < plot_limit:\n",
    "            print('\\n-- TEXTURE --')\n",
    "            print('Correlation: {:.3f}'.format(correlation))\n",
    "            print('Homogeneity: {:.3f}'.format(homogeneity))\n",
    "            print('Energy: {:.3f}'.format(energy))\n",
    "            print('Contrast: {:.3f}'.format(contrast))\n",
    "            print('Dissimilarity: {:.3f}'.format(dissimilarity))\n",
    "\n",
    "        if image_name in segmented_region_train:\n",
    "            if out_df:\n",
    "                data_list = train_list\n",
    "                train_index.append(image_name.split('.')[0])\n",
    "            else:\n",
    "                dataset = train\n",
    "        elif image_name in segmented_region_test:\n",
    "            if out_df:\n",
    "                data_list = test_list\n",
    "                test_index.append(image_name.split('.')[0])\n",
    "            else:\n",
    "                dataset = test\n",
    "        else:\n",
    "            print('Error: Cannot find {}'.format(image_name))\n",
    "            return None\n",
    "        \n",
    "        if out_df:\n",
    "            data_list.append([asymm_idx, ecc, compact_index, C_r, C_g, C_b,\n",
    "                              eq_diameter, correlation, homogeneity, energy, contrast,dissimilarity,ASM,localization])\n",
    "        else:\n",
    "            dataset[image_name] = [asymm_idx, ecc, compact_index, C_r, C_g, C_b,\n",
    "                                   eq_diameter, correlation, homogeneity, energy, contrast,dissimilarity,ASM,localization]\n",
    "\n",
    "    if out_df:\n",
    "        attr = ['AsymIdx', 'Eccentricity', 'CI', 'StdR', 'StdG', 'StdB', \n",
    "                'Diameter', 'Correlation', 'Homogeneity', 'Energy', 'Contrast',\"Dissimilarity\",\"ASM\",\"localization\"]\n",
    "        train_df = pd.DataFrame(data=train_list, index=train_index, columns=attr)\n",
    "        test_df = pd.DataFrame(data=test_list, index=test_index, columns=attr)\n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, class_labels = build_train_test(num_samples=10015,\n",
    "                                                   overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf363666",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8348726",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segmented_region_train, segmented_region_test = images_segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary version:\n",
    "features_train, features_test = features_extraction(segmented_region_train, segmented_region_test)\n",
    "\n",
    "train_names = list(features_train.keys())\n",
    "X_train = list(features_train.values())\n",
    "y_train = [train_df[train_df['image_id'] == img_name].reset_index().at[0, 'dx'] for img_name in train_names]\n",
    "\n",
    "test_names = list(features_test.keys())\n",
    "X_test = list(features_test.values())\n",
    "y_test = [test_df[test_df['image_id'] == img_name].reset_index().at[0, 'dx'] for img_name in test_names]\n",
    "\n",
    "# DataFrame version:\n",
    "#train_features_df, test_features_df = feature_extraction(segmented_region_train,\n",
    "                                                          #segmented_region_test,\n",
    "                                                          #out_df=True)\n",
    "# X_train = [list(train_features_df.iloc[row_idx])\n",
    "#            for row_idx in range(train_features_df.image_id.count())]\n",
    "# y_train = train_features_df.loc[:, 'dx']\n",
    "#\n",
    "# X_test = [list(test_features_df.iloc[row_idx])\n",
    "#           for row_idx in range(test_features_df.image_id.count())]\n",
    "# y_test = test_features_df.loc[:, 'dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99680728",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = svm.SVC(gamma='scale', class_weight='balanced')\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Training the SVM classifier...\")\n",
    "\n",
    "param_grid = {'C': [1, 1e1, 1e2, 1e3, 5e3, 1e4],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "              'class_weight': [None, 'balanced']}\n",
    "clf = GridSearchCV(svm.SVC(kernel='rbf'), param_grid)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best estimator found by Grid Search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***')\n",
    "# compute and plot performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "val_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "val_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "val_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(accuracy))\n",
    "print('F1-score: {:.3f}'.format(val_f1))\n",
    "print('Recall: {:.3f}'.format(val_recall))\n",
    "print('Precision: {:.3f}'.format(val_precision))\n",
    "\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_pred, target_names=class_labels))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d980668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainimgid=[]\n",
    "trainentries=[]\n",
    "testimgid=[]\n",
    "testentries=[]\n",
    "for k,v in features_train.items():\n",
    "    x=[]\n",
    "    x.append(k)\n",
    "    for i in v:\n",
    "        x.append(i)\n",
    "    trainentries.append(x)\n",
    "for k,v in features_test.items():\n",
    "    x=[]\n",
    "    x.append(k)\n",
    "    for i in v:\n",
    "        x.append(i)\n",
    "    testentries.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0207d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[['ImageID','AsymIdx', 'Eccentricity', 'CI', 'StdR', 'StdG', 'StdB', \n",
    "                'Diameter', 'Correlation', 'Homogeneity', 'Energy', 'Contrast',\"Dissimilarity\",\"ASM\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5962a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('train.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(features)\n",
    "    writer.writerows(trainentries)\n",
    "with open('test.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(features)\n",
    "    writer.writerows(testentries)\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df[\"Cancer_type\"] = y_train\n",
    "df.to_csv(\"train.csv\", index=False)\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "df[\"Cancer_type\"] = y_test\n",
    "df.to_csv(\"test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42717a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = list(features_train.keys())\n",
    "X_train = list(features_train.values())\n",
    "y_train = [train_df[train_df['image_id'] == img_name].reset_index().at[0, 'dx'] for img_name in train_names]\n",
    "\n",
    "test_names = list(features_test.keys())\n",
    "X_test = list(features_test.values())\n",
    "y_test = [test_df[test_df['image_id'] == img_name].reset_index().at[0, 'dx'] for img_name in test_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff5e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
